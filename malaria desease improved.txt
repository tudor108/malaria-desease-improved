# 1. Importuri necesare
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Input, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.metrics import TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Precision, Recall
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import albumentations as A
from albumentations.core.composition import OneOf
from albumentations.tensorflow import ToTensorV2
from tensorflow.keras.applications import ResNet50

# Pentru MLOps
import wandb
from wandb.keras import WandbCallback

# 2. Inițializare Weight & Biases
wandb.init(project="malaria-diagnosis")

# 3. Descărcare dataset
DATASET_NAME = "malaria"
dataset, dataset_info = tfds.load(DATASET_NAME, with_info=True, as_supervised=True)

# 4. Funcție de împărțire a dataset-ului
def splits(dataset, train_ratio, val_ratio, test_ratio):
    dataset = dataset.shuffle(1000)
    total_size = tf.data.experimental.cardinality(dataset).numpy()

    train_size = int(train_ratio * total_size)
    val_size = int(val_ratio * total_size)

    train_dataset = dataset.take(train_size)
    remaining = dataset.skip(train_size)

    val_dataset = remaining.take(val_size)
    test_dataset = remaining.skip(val_size)

    return train_dataset, val_dataset, test_dataset

# 5. Împărțirea dataset-ului
TRAIN_RATIO = 0.8
VAL_RATIO = 0.1
TEST_RATIO = 0.1
train_dataset, val_dataset, test_dataset = splits(dataset["train"], TRAIN_RATIO, VAL_RATIO, TEST_RATIO)

# 6. Redimensionare și normalizare
IM_SIZE = 224
BATCH_SIZE = 32

@tf.function
def resize_rescale(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, [IM_SIZE, IM_SIZE])
    return image, label

train_dataset = (
    train_dataset.map(resize_rescale, num_parallel_calls=tf.data.AUTOTUNE)
    .shuffle(1000)
    .batch(BATCH_SIZE)
    .prefetch(tf.data.AUTOTUNE)
)
val_dataset = (
    val_dataset.map(resize_rescale, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(BATCH_SIZE)
    .prefetch(tf.data.AUTOTUNE)
)
test_dataset = (
    test_dataset.map(resize_rescale, num_parallel_calls=tf.data.AUTOTUNE)
    .batch(BATCH_SIZE)
    .prefetch(tf.data.AUTOTUNE)
)

# 7. Callback-uri personalizate
checkpoint_cb = ModelCheckpoint("best_model.h5", save_best_only=True)
def scheduler(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * tf.math.exp(-0.1)
lr_scheduler_cb = LearningRateScheduler(scheduler)
early_stopping_cb = EarlyStopping(patience=5, restore_best_weights=True)

# 8. Feature Extraction cu Functional API
base_model = ResNet50(weights="imagenet", include_top=False, input_shape=(IM_SIZE, IM_SIZE, 3))
base_model.trainable = False

inputs = tf.keras.Input(shape=(IM_SIZE, IM_SIZE, 3))
x = base_model(inputs, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dropout(0.5)(x)
outputs = tf.keras.layers.Dense(1, activation="sigmoid")(x)
feature_extraction_model = tf.keras.Model(inputs, outputs)

feature_extraction_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss=BinaryCrossentropy(),
    metrics=[Precision(), Recall(), TruePositives(), TrueNegatives(), FalsePositives(), FalseNegatives()]
)

# 9. Model Subclassing
class CustomModel(tf.keras.Model):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.conv1 = Conv2D(32, (3, 3), activation="relu")
        self.pool1 = MaxPool2D()
        self.conv2 = Conv2D(64, (3, 3), activation="relu")
        self.pool2 = MaxPool2D()
        self.flatten = Flatten()
        self.fc1 = Dense(128, activation="relu")
        self.dropout = Dropout(0.5)
        self.fc2 = Dense(1, activation="sigmoid")

    def call(self, inputs):
        x = self.conv1(inputs)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.dropout(x)
        return self.fc2(x)

custom_model = CustomModel()
custom_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss=BinaryCrossentropy(),
    metrics=[Precision(), Recall()]
)

# 10. Augmentări avansate
augmentations = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    OneOf([
        A.RandomBrightnessContrast(p=0.5),
        A.HueSaturationValue(p=0.5)
    ], p=0.5),
    ToTensorV2()
])

def augment(image, label):
    image = tf.numpy_function(
        func=lambda img: augmentations(image=img.numpy())['image'],
        inp=[image],
        Tout=tf.float32
    )
    return image, label

train_dataset = train_dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)

# 11. Antrenare model Sequential cu augmentări
history = feature_extraction_model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=20,
    callbacks=[checkpoint_cb, lr_scheduler_cb, early_stopping_cb, WandbCallback()]
)

# 12. Evaluare pe setul de test
test_loss, test_metrics = feature_extraction_model.evaluate(test_dataset, verbose=1)
print(f"Test Loss: {test_loss}")

# 13. Matrice de confuzie
predictions = feature_extraction_model.predict(test_dataset)
binary_predictions = (predictions > 0.5).astype(int)
test_images, test_labels = next(iter(test_dataset))
conf_matrix = confusion_matrix(test_labels.numpy(), binary_predictions)

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.ylabel("Actual Labels")
plt.xlabel("Predicted Labels")
plt.show()

# 14. ROC și AUC
fpr, tpr, _ = roc_curve(test_labels.numpy(), predictions)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC curve (area = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic")
plt.legend(loc="lower right")
plt.show()

# 15. MLOps cu Weights & Biases
wandb.log({
    "Test Loss": test_loss,
    "ROC AUC": roc_auc
})
